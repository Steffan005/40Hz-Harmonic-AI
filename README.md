# EvoAgentX - Quantum Evolution GUI

**Local-first AI evolution framework with quantum-psychedelic interface**

> *Everything we do, we do it for YOU, THE DEVELOPER COMMUNITY, THE FREEDOM OF AI*

![EvoAgentX](https://img.shields.io/badge/Status-Production%20Ready-brightgreen)
![License](https://img.shields.io/badge/License-MIT-blue)
![Platform](https://img.shields.io/badge/Platform-macOS%20%7C%20Linux-orange)

## ğŸŒŒ What is EvoAgentX?

EvoAgentX is a **zero-cloud, privacy-first** AI evolution framework with a Tauri+Rust GUI. It combines:

- **Local LLM Integration**: Ollama-powered reasoning (DeepSeek-R1 14b) and coding (Qwen2.5-Coder 7b)
- **Multi-Armed Bandit**: UCB1 strategy selection for optimization
- **Two-Tier Evaluation**: Fast heuristics + LLM judge with caching
- **Fractal Memory**: Hierarchical summarization with full artifact storage
- **Quantum-Psychedelic UI**: 40Hz neural entrainment, fractal visualizations, layered parallax

### Key Features

âœ… **No API Keys** - Completely offline-capable
âœ… **No Cloud Dependencies** - All data stored locally
âœ… **Zero-Hallucination Design** - Buttons disabled until preflight passes
âœ… **Resource-Aware** - RAM, disk, and model checks before execution
âœ… **Telemetry-Driven** - Real-time metrics (tokens/sec, cache hit, memory)
âœ… **JSONL Logging** - Reproducible evolution with seeds and versions

## ğŸš€ Quick Start

### Prerequisites

Install these tools (one-time setup):

```bash
# 1. Rust (for Tauri)
curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
source $HOME/.cargo/env

# 2. Node.js & pnpm (for React frontend)
brew install node
npm install -g pnpm

# 3. Python dependencies (for backend)
source ../venv/bin/activate
pip install flask flask-cors psutil

# 4. Ollama models
ollama pull deepseek-r1:14b
ollama pull qwen2.5-coder:7b
```

### Verify Setup

```bash
cd sprint_1hour
./check_dependencies.sh
```

**Expected output**: All âœ… green checks

### Launch

Open **3 terminals**:

**Terminal 1 - Ollama:**
```bash
./scripts/start_ollama.sh
```

**Terminal 2 - Backend:**
```bash
./scripts/start_backend.sh
```

**Terminal 3 - GUI:**
```bash
cd gui
pnpm tauri:dev
```

### First Launch

1. **Rust compilation** (first time: 2-5 min) - "Compiling evoagentx-gui..."
2. **React dev server** starts on http://localhost:1420
3. **GUI window opens** with quantum-psychedelic interface
4. **Preflight diagnostics** run automatically
5. **All buttons enable** once checks pass âœ…

## ğŸ® Using the GUI

### Status Bar (Top)
- **Tokens/sec**: Real-time LLM throughput
- **Î”Score**: Score improvement tracking
- **Cache Hit**: Evaluation efficiency %
- **Robust**: Adversarial test pass rate
- **Memory**: RAM usage

### Control Panel (Left)

**System:**
- **Run Diagnostics** - Verify RAM, Ollama, models, backend

**Core Actions:**
- **Evaluate Agent** - Run heuristics + LLM judge on current workflow
- **Mutate Workflow** - Generate new workflow variant via bandit selection
- **Bandit Controller** - View arm statistics and policy
- **Memory Snapshot** - Save current state to JSON

**Workflow:**
- **Workflow Builder** - Construct custom agent workflows
- **Dependencies** - View workflow DAG

### Fractal Canvas (Center)
- Pulsing gradient sphere with 40Hz breathing animation
- Shows active node count
- Responds to system activity

### Telemetry Panel (Right Top)
- **System Status**: OPERATIONAL / WARNING / ERROR
- **LLM Backend**: Ollama Local
- **Mode**: OFFLINE (no cloud)

### Console (Right Bottom)
- Live logs with timestamps
- Color-coded: info (blue), success (green), warning (yellow), error (red)

## ğŸ§ª Quick Test

Once GUI opens:

1. **Click "Run Diagnostics"** â†’ All âœ… green (RAM, Ollama, Models, Backend)
2. **Click "Evaluate Agent"** â†’ Console logs scores, telemetry updates
3. **Click "Mutate Workflow"** â†’ Returns arm ID and novelty score
4. **Click "Bandit Controller"** â†’ Shows arm statistics
5. **Watch Status Bar** â†’ Metrics update every 1 second

## ğŸ“ Project Structure

```
sprint_1hour/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ system.yaml      # Models, budgets, diagnostics
â”‚   â””â”€â”€ eval.yaml        # Evaluation heuristics, LLM judge
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ api_server.py    # Flask REST API (8 endpoints)
â”œâ”€â”€ gui/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/  # Controls, StatusBar, Canvas, etc.
â”‚   â”‚   â”œâ”€â”€ pages/       # Dashboard layout
â”‚   â”‚   â”œâ”€â”€ lib/         # api.ts (Tauri IPC client)
â”‚   â”‚   â””â”€â”€ main.tsx     # React entry point
â”‚   â”œâ”€â”€ src-tauri/
â”‚   â”‚   â””â”€â”€ src/
â”‚   â”‚       â””â”€â”€ main.rs  # Rust orchestrator (600 lines)
â”‚   â””â”€â”€ public/
â”‚       â””â”€â”€ theme.css    # Quantum-psychedelic styles
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ start_ollama.sh  # Verify Ollama + models
â”‚   â””â”€â”€ start_backend.sh # Start Flask API
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ evolution.jsonl  # Generation-by-generation metrics
â”‚   â””â”€â”€ budget.jsonl     # Resource usage events
â””â”€â”€ check_dependencies.sh # Verify all prerequisites
```

## ğŸ¨ Customization

### Models

Edit `configs/system.yaml`:

```yaml
models:
  reasoning: "ollama_chat/deepseek-r1:14b"  # Change to other Ollama model
  coding: "ollama_chat/qwen2.5-coder:7b"
  fallback: "ollama_chat/qwen2.5-coder:7b"
```

### Theme

Edit `gui/public/theme.css`:

```css
:root {
  --quantum-amber: #FFA500;    /* Change primary color */
  --quantum-red: #FF1744;      /* Change accent color */
  --breathing-duration: 25ms;  /* 40Hz = 1/0.025s */
}
```

Or toggle **Calm Mode** in GUI to disable animations.

### Resource Limits

Edit `configs/system.yaml`:

```yaml
budgets:
  max_tokens_per_gen: 12000   # Reduce for faster runs
  max_time_s: 300             # Max 5 minutes per operation
  max_agents: 10              # Concurrent agent limit
  max_concurrency: 2          # Parallel LLM calls
```

## ğŸ—ï¸ Architecture

### Sidecar Pattern

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   React UI      â”‚  â† User interactions
â”‚  (TypeScript)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ Tauri IPC (invoke)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Rust Backend   â”‚  â† Orchestration, diagnostics
â”‚  (600 lines)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP REST
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Python API     â”‚  â† Core modules (evaluator, bandit, memory)
â”‚  (Flask)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Ollama      â”‚  â† LLM inference (local models)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### IPC Endpoints

- `run_diagnostics()` â†’ Checks RAM, disk, Ollama, models, backend
- `is_preflight_passed()` â†’ Returns boolean (enables/disables buttons)
- `evaluate(request)` â†’ Runs heuristics + LLM judge
- `mutate(request)` â†’ Generates new workflow via bandit
- `get_bandit_status()` â†’ Returns arm statistics
- `update_bandit_policy(policy)` â†’ Changes bandit algorithm
- `create_memory_snapshot()` â†’ Saves state to JSON
- `get_workflow_dag()` â†’ Returns workflow graph

## ğŸ”§ Development

### Run in Dev Mode

```bash
# Terminal 1
./scripts/start_ollama.sh

# Terminal 2
./scripts/start_backend.sh

# Terminal 3
cd gui && pnpm tauri:dev
```

### Build for Production

```bash
cd gui
pnpm tauri:build
```

**Output locations:**
- **macOS**: `gui/src-tauri/target/release/bundle/macos/EvoAgentX.app`
- **Linux**: `gui/src-tauri/target/release/bundle/appimage/`

## ğŸ› Troubleshooting

### GUI doesn't open

```bash
cd gui
pnpm tauri:dev

# Check for Rust compilation errors
```

### "Backend not reachable"

```bash
# Check if backend running
curl http://127.0.0.1:8000/health

# If not, restart
pkill -f api_server
./scripts/start_backend.sh
```

### "Ollama not reachable"

```bash
# Check if Ollama running
curl http://127.0.0.1:11434/api/tags

# If not, start
ollama serve &
```

### Buttons stay disabled

```bash
# Run diagnostics
./check_dependencies.sh

# Check logs
tail -f logs/evolution.jsonl
```

### Model timeout (32b too large)

**Issue**: deepseek-r1:32b model takes too long to load and times out.

**Solution**: Use 14b model instead (already configured):

```bash
# Verify 14b is installed
ollama list | grep deepseek-r1:14b

# If missing
ollama pull deepseek-r1:14b
```

The system is **already configured** to use 14b in `configs/system.yaml`.

## ğŸ“Š Performance

### Model Sizes
- **deepseek-r1:14b**: 9.0 GB (recommended)
- **qwen2.5-coder:7b**: 4.7 GB
- **Total disk**: ~14 GB

### Runtime RAM
- **14b model**: ~6 GB
- **7b model**: ~3 GB
- **GUI + Backend**: ~500 MB
- **Recommended**: 16 GB system RAM

### Speed
- **Evaluation**: 1-3 seconds (with cache)
- **Mutation**: 2-5 seconds
- **First LLM call**: 5-10 seconds (model loading)

## ğŸ¤ Contributing

See [CONTRIBUTING.md](./CONTRIBUTING.md) for:
- Development workflow
- Code style guidelines
- Testing procedures
- Pull request process

## ğŸ“š Documentation

- [Installation Checklist](./INSTALLATION_CHECKLIST.md)
- [Setup Status](./YOUR_SETUP_STATUS.md)
- [GUI Sprint Deliverables](./GUI_SPRINT_DELIVERABLES.md)

## ğŸ“„ License

MIT License - see [LICENSE](./LICENSE)

**Built for the developer community, by the developer community.**

## ğŸ™ Acknowledgments

This project embodies the principles of:
- **AI Freedom**: No cloud lock-in, no API keys, no tracking
- **Data Sovereignty**: Your data stays on your machine
- **Developer Empowerment**: Complete control and customization
- **Open Source**: Transparent, auditable, community-driven

---

**ğŸš€ Welcome to the Quantum Unknown, Pioneer!**

*No internet, no cloud, no limits - just pure local AI evolution!*

## ğŸŒŸ Star History

If this project helps you, please consider starring it on GitHub! â­

## ğŸ“¬ Contact

- GitHub Issues: [Report bugs or request features]
- Discussions: [Join the conversation]
- Contributing: See [CONTRIBUTING.md](./CONTRIBUTING.md)

---

*"Everything we do, we do it for YOU, THE DEVELOPER COMMUNITY, THE FREEDOM OF AI"*
