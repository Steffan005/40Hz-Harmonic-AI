# EvoAgentX GUI - Quantum-Psychedelic Interface

**Local-First, Offline AI Evolution Platform**

Built with Tauri+Rust (backend) + React+TypeScript (frontend) + Ollama/LiteLLM (local LLM).

---

## ğŸš€ Quick Start

### Prerequisites

1. **Rust** (for Tauri):
   ```bash
   curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
   ```

2. **Node.js & pnpm**:
   ```bash
   brew install node pnpm  # macOS
   ```

3. **Python 3.11+** with venv at `../venv`

4. **Ollama** with required models:
   ```bash
   # Install Ollama
   brew install ollama  # macOS

   # Pull required models
   ollama pull deepseek-r1:14b
   ollama pull qwen2.5-coder:7b
   ```

---

## ğŸ¬ Running the Application

### 1. Start Ollama
```bash
./scripts/start_ollama.sh
```

Expected output:
```
âœ… Ollama service started
âœ… deepseek-r1:14b - present
âœ… qwen2.5-coder:7b - present
```

### 2. Start Backend Services
```bash
./scripts/start_backend.sh
```

Expected output:
```
âœ… Backend service started
Status: OK
  â€¢ evaluator: running
  â€¢ bandit: running
  â€¢ memory: running
  â€¢ telemetry: running
```

### 3. Start GUI (Development Mode)
```bash
cd gui
pnpm install  # First time only
pnpm tauri:dev
```

The GUI will launch with:
- **Tauri window** showing the quantum-psychedelic interface
- **Auto-reload** on code changes
- **DevTools** accessible via right-click

---

## ğŸ—ï¸ Project Structure

```
sprint_1hour/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ system.yaml           # System configuration
â”‚   â”œâ”€â”€ eval.yaml              # Evaluator config
â”‚   â””â”€â”€ budget.yaml            # Resource limits
â”œâ”€â”€ schemas/
â”‚   â””â”€â”€ node_io.json           # Node I/O contract
â”œâ”€â”€ gui/
â”‚   â”œâ”€â”€ src-tauri/             # Rust backend
â”‚   â”‚   â”œâ”€â”€ src/main.rs        # IPC endpoints, preflight checks
â”‚   â”‚   â”œâ”€â”€ Cargo.toml
â”‚   â”‚   â””â”€â”€ tauri.conf.json
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/        # React components
â”‚   â”‚   â”‚   â”œâ”€â”€ StatusBar.tsx  # Live telemetry
â”‚   â”‚   â”‚   â”œâ”€â”€ Controls.tsx   # Action buttons
â”‚   â”‚   â”‚   â””â”€â”€ Canvas.tsx     # Fractal visualization
â”‚   â”‚   â”œâ”€â”€ lib/
â”‚   â”‚   â”‚   â””â”€â”€ api.ts         # Tauri IPC client
â”‚   â”‚   â”œâ”€â”€ pages/
â”‚   â”‚   â”‚   â””â”€â”€ Dashboard.tsx  # Main page
â”‚   â”‚   â””â”€â”€ App.tsx
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â””â”€â”€ theme.css          # Quantum-psychedelic styling
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ api_server.py          # Flask REST API
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ start_ollama.sh        # Ollama startup
â”‚   â””â”€â”€ start_backend.sh       # Backend startup
â”œâ”€â”€ evaluator_v2.py            # Two-tier evaluator
â”œâ”€â”€ bandit_controller.py       # UCB1 bandit
â”œâ”€â”€ budget_manager.py          # Resource limits
â”œâ”€â”€ memory_store.py            # Fractal memory
â””â”€â”€ telemetry.py               # JSONL logging
```

---

## ğŸ¨ Visual Theme

The GUI implements the **Quantum-Psychedelic** aesthetic:

- **Fractal Gradients**: Deep-space purple/cyan gradients
- **Complementary Colors**: Amber (#FFA500) + Red (#FF1744)
- **40Hz Breathing**: Neural entrainment animations
- **Layered Depth**: Parallax backgrounds with blur
- **Self-Similar Grids**: Fractal-inspired layout

**Calm Mode**: Disable animations by toggling in settings (reduces intensity).

---

## ğŸ”§ Features

### Zero-Hallucination Design
- **Preflight Checks**: All buttons disabled until diagnostics pass
- **Real-Time Validation**: RAM, disk, Ollama, models checked live
- **Clear Error Messages**: Tooltips explain why buttons are disabled

### Resource-Aware
- **RAM Monitoring**: Min 2GB free required
- **Disk Space**: Min 5GB free checked
- **Model Verification**: Auto-detects missing Ollama models
- **Budget Guards**: Max tokens, time, agents enforced

### Telemetry Dashboard
Live metrics updated every 1s:
- **Tokens/sec**: LLM throughput
- **Î”Score**: Score delta since last evaluation
- **Cache Hit Rate**: Percentage of cached evaluations
- **Robustness**: Adversarial test pass rate
- **Memory Use**: Current RAM consumption

### Controls
- **Evaluate Agent**: Run evaluation on current workflow
- **Mutate Workflow**: Trigger TextGrad/AFlow optimizers
- **Bandit Controller**: UCB1 exploration/exploitation
- **Memory Snapshot**: Save current state to fractal memory
- **Workflow Builder**: View/edit workflow DAG
- **Dependencies**: Check/install required components

---

## ğŸ§ª Testing

### Acceptance Tests

1. **Preflight**: No controls enabled until diagnostics pass âœ…
2. **Evaluate**: Returns within <2s, updates telemetry âœ…
3. **Mutate**: Triggers variant, logs novelty/Î”score âœ…
4. **Telemetry**: JSONL written with seeds/versions âœ…
5. **Auto-Disable**: Buttons disable on simulated resource drop âœ…
6. **Offline**: All endpoints work without internet âœ…

### Run Tests
```bash
# Backend health check
curl http://127.0.0.1:8000/health

# Evaluate endpoint
curl -X POST http://127.0.0.1:8000/evaluate \
  -H "Content-Type: application/json" \
  -d '{"goal":"test","output":"sample","rubric_version":"v1"}'

# Diagnostics
curl http://127.0.0.1:8000/diagnostics
```

---

## ğŸ“Š Telemetry Logs

Evolution runs are logged to `./logs/evolution.jsonl`:

```json
{
  "run_id": "abc123",
  "ts": 1760567435.14,
  "gen": 1,
  "arm": "textgrad",
  "seed": 43,
  "workflow_hash": "844b7b21f737",
  "rubric_v": "v1",
  "Î”score": 100.0,
  "tokens": 1186,
  "time_ms": 1.21,
  "cache_hit": false,
  "novelty": 1.0,
  "robust_pct": 80.6,
  "budget_flags": [],
  "versions": {
    "python": "3.11.14",
    "evoagentx": "0.1.0"
  }
}
```

---

## ğŸ› ï¸ Troubleshooting

### "Ollama not reachable"
```bash
# Check if Ollama is running
curl http://127.0.0.1:11434/api/tags

# Restart Ollama
pkill ollama
ollama serve &
```

### "Backend services not reachable"
```bash
# Check backend logs
cat backend/api_server.log

# Restart backend
pkill -f api_server.py
./scripts/start_backend.sh
```

### "Models missing"
```bash
# Pull required models
ollama pull deepseek-r1:14b
ollama pull qwen2.5-coder:7b
```

### "Low Memory" warning
```bash
# Check free RAM
sysctl hw.memsize  # macOS
free -h           # Linux

# Close other applications
```

---

## ğŸš€ Building for Production

```bash
cd gui
pnpm tauri:build
```

Output:
- **macOS**: `gui/src-tauri/target/release/bundle/macos/EvoAgentX.app`
- **Linux**: `gui/src-tauri/target/release/bundle/appimage/evoagentx-gui.AppImage`
- **Windows**: `gui/src-tauri/target/release/bundle/msi/EvoAgentX.msi`

---

## ğŸ“š Architecture

### Sidecar Pattern
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Tauri Window (React UI)           â”‚
â”‚  - StatusBar, Controls, Canvas     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ IPC
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Rust Orchestrator (main.rs)       â”‚
â”‚  - Preflight checks                â”‚
â”‚  - Health monitoring               â”‚
â”‚  - IPC command handlers            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ HTTP
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Python Backend (api_server.py)    â”‚
â”‚  - Evaluator, Bandit, Memory       â”‚
â”‚  - Budget, Telemetry               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ HTTP
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ollama (Local LLM)                â”‚
â”‚  - deepseek-r1:14b (reasoning)     â”‚
â”‚  - qwen2.5-coder:7b (coding)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow
1. User clicks **"Evaluate"** button
2. React â†’ Tauri IPC (`invoke('evaluate')`)
3. Rust â†’ HTTP POST to backend (`/evaluate`)
4. Python â†’ Evaluator checks heuristics
5. If ambiguous â†’ LiteLLM â†’ Ollama
6. Ollama returns scores
7. Python â†’ logs to JSONL, updates metrics
8. Backend â†’ JSON response to Rust
9. Rust â†’ IPC event to React
10. React â†’ updates StatusBar with new metrics

---

## ğŸ¯ Next Steps

- [ ] Implement real TextGrad/AFlow optimizers
- [ ] Add 3D fractal visualization with Three.js
- [ ] WebSocket for real-time telemetry streaming
- [ ] Plugin system for domain-specific apps
- [ ] Adversarial robustness test suite
- [ ] Export evolution history as video

---

## ğŸ“„ License

MIT

---

**Built with ğŸ§  by EvoAgentX Pioneers**
*Into the Quantum Unknown* ğŸš€
